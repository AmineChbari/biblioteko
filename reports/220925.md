# 22/09/25

## Mise en place du repo git
Fork du repo principal (biblioteka), mise en place de nos fichiers, de l'architecture du projet, de l'environnement et des règles à appliquer.

## Mise en place de l'API Gemini sur Python
Nous avons mis en place de quoi utiliser l'API Gemini sur Python.
La clé est stockée sur un .env volatile (dans le .gitignore).

## Utilisation de l'API Gemini
Nous avons envoyé la première de couverture (en version brute) du livre exemple à analyser, et le modèle (Gemini 2.5 flash) a correctement su identifier le titre, l'éditeur, l'auteur etc

Par la suite nous avons envoyé la 2e page (le sommaire) en version brute et en version nettoyée.
Le modèle détecte un peu mieux le texte lorsque l'image est bien orientée, mais il a toujours du mal à lire du texte flou et à se rendre compte que le texte est flou.

Nous avons ensuite utilisé le modèle gemini-2.5-flash-preview-05-20 qui semble empiriquement un peu plus fiable (car plus récent?).

## Utilisation de PyMuPDF
Nous avons utilisé PyMuPDF pour analyser un PDF complet.
Les images obtenues sont de bien meilleure résolution et les LLM ont tendance à mieux pouvoir extraire le texte même s'il est flou ou incomplet.

## Remarques diverses

- Le formatage des images est important, des images zoomées et précises sont plus faciles à analyser
- Les LLM ont du mal à dire qu'ils ne savent pas faire ou ne savent pas répondre
- Le prompt joue un rôle majeur lorsqu'il s'agit de bien formater le texte de sortie et pour aiguiller le modèle sur la lecture et la compréhension de l'image

## Pistes d'amélioration

- Pour s'assurer de la fiabilité des résultats, nous envisageons d'utiliser plusieurs modèles de LLM et peut-être aussi des librairies d'OCR plus basiques pour vérifier avec une certaine redondance que les résultats sont cohérents (si 3 modèles/librairies donnent le même résultat, c'est que c'est très probablement correct)
- Utiliser du threading (await/async en Python car contrainte I/O longue) pour lancer les appels d'API en arrière-plan sans bloquer le back-end